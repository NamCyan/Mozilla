Dump name space
results/maven/FairGrad/perm_4/options.json
Dump name space
results/maven/FairGrad/perm_4/options.json
Dump name space
results/maven/FairGrad/perm_4/options.json
Dump name space
results/maven/FairGrad/perm_4/options.json
[[157, 18, 114, 92, 67, 153, 80, 115, 78, 154, 27, 5, 69, 117, 44, 83, 155, 40, 45, 139, 137, 14, 138, 119, 74, 150, 59, 54, 28, 9, 140, 37, 107, 76, 47, 0], [151, 16, 3, 70, 106, 141, 68, 31, 79, 101, 166, 167, 94, 21, 51, 7, 73, 145, 2, 124, 159, 61, 41, 142, 126, 144, 55, 168, 29, 11, 24, 0], [134, 99, 105, 91, 52, 19, 135, 122, 127, 102, 108, 104, 110, 38, 118, 121, 160, 26, 34, 33, 158, 130, 165, 86, 149, 146, 164, 4, 131, 75, 90, 10, 32, 0], [46, 36, 17, 13, 12, 20, 25, 49, 169, 148, 87, 56, 95, 81, 58, 57, 161, 64, 163, 82, 43, 125, 35, 96, 156, 100, 66, 30, 50, 120, 0], [147, 48, 84, 39, 53, 23, 109, 22, 77, 62, 93, 143, 89, 8, 71, 85, 63, 97, 65, 103, 113, 112, 123, 152, 88, 98, 72, 42, 60, 133, 128, 116, 15, 111, 136, 162, 129, 132, 6, 0]]
True
train loader and exclude loader:  0
Original len train instances: 347788
train loader and exclude loader:  1
Original len train instances: 334375
train loader and exclude loader:  2
Original len train instances: 319808
train loader and exclude loader:  3
Original len train instances: 306881
train loader and exclude loader:  4
Original len train instances: 294589
Original len dev instances: 57198
Original len test instances: 98603
create basgMTL 4
False True
Epoch   1  Train Loss 0.7084 94.60%
Epoch   1:  Dev 0.49625164270401
Epoch   1: Test 0.5248839259147644
patience: 0 / 5
Epoch   2  Train Loss 0.4764 95.14%
Epoch   2:  Dev 0.522988498210907
Epoch   2: Test 0.541880190372467
patience: 0 / 5
Epoch   3  Train Loss 0.4276 95.35%
Epoch   3:  Dev 0.5260974764823914
Epoch   3: Test 0.5504958033561707
patience: 0 / 5
Epoch   4  Train Loss 0.3957 95.52%
Epoch   4:  Dev 0.517357587814331
patience: 1 / 5
Epoch   5  Train Loss 0.3683 95.66%
Epoch   5:  Dev 0.5207218527793884
patience: 2 / 5
Epoch   6  Train Loss 0.3450 95.80%
Epoch   6:  Dev 0.5359049439430237
Epoch   6: Test 0.5560935735702515
patience: 0 / 5
Epoch   7  Train Loss 0.3250 95.99%
Epoch   7:  Dev 0.5277500748634338
patience: 1 / 5
Epoch   8  Train Loss 0.3064 96.09%
Epoch   8:  Dev 0.5296617150306702
patience: 2 / 5
Epoch   9  Train Loss 0.2902 96.28%
Epoch   9:  Dev 0.542932391166687
Epoch   9: Test 0.5566290020942688
patience: 0 / 5
Epoch  10  Train Loss 0.2754 96.42%
Epoch  10:  Dev 0.532329797744751
patience: 1 / 5
Epoch  11  Train Loss 0.2629 96.66%
Epoch  11:  Dev 0.5356705188751221
patience: 2 / 5
Epoch  12  Train Loss 0.2516 96.83%
Epoch  12:  Dev 0.5502852201461792
Epoch  12: Test 0.5632016658782959
patience: 0 / 5
Epoch  13  Train Loss 0.2436 97.00%
Epoch  13:  Dev 0.5479111075401306
patience: 1 / 5
Epoch  14  Train Loss 0.2338 97.14%
Epoch  14:  Dev 0.5414113402366638
patience: 2 / 5
Epoch  15  Train Loss 0.2267 97.27%
Epoch  15:  Dev 0.5506829619407654
Epoch  15: Test 0.5624476075172424
patience: 0 / 5
setting train exemplar for learned classes
torch.Size([700, 2048])
BEST DEV 0: 0.5506829619407654
BEST TEST 0: 0.5624476075172424
torch.Size([700, 2048])
Epoch   1  Train Loss 8.9349 92.13%
Epoch   1:  Dev 0.565743625164032
Epoch   1: Test 0.5614035129547119
patience: 0 / 5
Epoch   2  Train Loss 11.2335 93.65%
Epoch   2:  Dev 0.5870370268821716
Epoch   2: Test 0.5750563740730286
patience: 0 / 5
Epoch   3  Train Loss 12.9591 93.99%
Epoch   3:  Dev 0.5935681462287903
Epoch   3: Test 0.5905582904815674
patience: 0 / 5
Epoch   4  Train Loss 14.3939 94.22%
Epoch   4:  Dev 0.5926758050918579
patience: 1 / 5
Epoch   5  Train Loss 15.6715 94.40%
Epoch   5:  Dev 0.5953549146652222
Epoch   5: Test 0.5878148078918457
patience: 0 / 5
Epoch   6  Train Loss 16.8196 94.54%
Epoch   6:  Dev 0.5976665616035461
Epoch   6: Test 0.5890524983406067
patience: 0 / 5
Epoch   7  Train Loss 17.9641 94.70%
Epoch   7:  Dev 0.5933103561401367
patience: 1 / 5
Epoch   8  Train Loss 19.0591 94.82%
Epoch   8:  Dev 0.5903132557868958
patience: 2 / 5
Epoch   9  Train Loss 20.4869 94.92%
Epoch   9:  Dev 0.5952513217926025
patience: 3 / 5
Epoch  10  Train Loss 21.7478 95.00%
Epoch  10:  Dev 0.5995834469795227
Epoch  10: Test 0.5918792486190796
patience: 0 / 5
Epoch  11  Train Loss 23.1547 95.12%
Epoch  11:  Dev 0.6019529104232788
Epoch  11: Test 0.5916182398796082
patience: 0 / 5
Epoch  12  Train Loss 24.6058 95.25%
Epoch  12:  Dev 0.5995475053787231
patience: 1 / 5
Epoch  13  Train Loss 26.0613 95.35%
Epoch  13:  Dev 0.5907006859779358
patience: 2 / 5
Epoch  14  Train Loss 27.5674 95.39%
Epoch  14:  Dev 0.59945148229599
patience: 3 / 5
Epoch  15  Train Loss 29.5572 95.52%
Epoch  15:  Dev 0.5982776880264282
patience: 4 / 5
setting train exemplar for learned classes
torch.Size([1320, 2048])
BEST DEV 1: 0.6019529104232788
BEST TEST 1: 0.5916182398796082
torch.Size([1320, 2048])
Epoch   1  Train Loss 9.0662 90.93%
Epoch   1:  Dev 0.6123470067977905
Epoch   1: Test 0.6016266345977783
patience: 0 / 5
Epoch   2  Train Loss 10.7835 93.27%
Epoch   2:  Dev 0.615013062953949
Epoch   2: Test 0.6155362129211426
patience: 0 / 5
Epoch   3  Train Loss 11.8376 93.60%
Epoch   3:  Dev 0.619530439376831
Epoch   3: Test 0.6207229495048523
patience: 0 / 5
Epoch   4  Train Loss 12.7843 93.82%
Epoch   4:  Dev 0.6240882873535156
Epoch   4: Test 0.6235133409500122
patience: 0 / 5
Epoch   5  Train Loss 13.5811 94.03%
Epoch   5:  Dev 0.6204136610031128
patience: 1 / 5
Epoch   6  Train Loss 14.3090 94.14%
Epoch   6:  Dev 0.6212131381034851
patience: 2 / 5
Epoch   7  Train Loss 15.0120 94.31%
Epoch   7:  Dev 0.6195191144943237
patience: 3 / 5
Epoch   8  Train Loss 15.7566 94.39%
Epoch   8:  Dev 0.6174170970916748
patience: 4 / 5
Epoch   9  Train Loss 16.3940 94.51%
Epoch   9:  Dev 0.6187882423400879
patience: 5 / 5
setting train exemplar for learned classes
torch.Size([1980, 2048])
BEST DEV 2: 0.6240882873535156
BEST TEST 2: 0.6235133409500122
torch.Size([1980, 2048])
Epoch   1  Train Loss 11.4875 89.60%
Epoch   1:  Dev 0.6035944819450378
Epoch   1: Test 0.6107008457183838
patience: 0 / 5
Epoch   2  Train Loss 12.9387 92.25%
Epoch   2:  Dev 0.6096329689025879
Epoch   2: Test 0.6156597137451172
patience: 0 / 5
Epoch   3  Train Loss 14.0428 92.49%
Epoch   3:  Dev 0.6084163188934326
patience: 1 / 5
Epoch   4  Train Loss 14.8985 92.71%
Epoch   4:  Dev 0.6072307229042053
patience: 2 / 5
Epoch   5  Train Loss 15.6926 92.93%
Epoch   5:  Dev 0.6130631566047668
Epoch   5: Test 0.6202870011329651
patience: 0 / 5
Epoch   6  Train Loss 16.4601 93.08%
Epoch   6:  Dev 0.6047980785369873
patience: 1 / 5
Epoch   7  Train Loss 17.1398 93.15%
Epoch   7:  Dev 0.604834794998169
patience: 2 / 5
Epoch   8  Train Loss 17.8629 93.30%
Epoch   8:  Dev 0.6070461273193359
patience: 3 / 5
Epoch   9  Train Loss 18.5481 93.41%
Epoch   9:  Dev 0.6016319990158081
patience: 4 / 5
Epoch  10  Train Loss 19.1327 93.50%
Epoch  10:  Dev 0.6014907956123352
patience: 5 / 5
setting train exemplar for learned classes
torch.Size([2580, 2048])
BEST DEV 3: 0.6130631566047668
BEST TEST 3: 0.6202870011329651
torch.Size([2580, 2048])
Epoch   1  Train Loss 13.3158 88.19%
Epoch   1:  Dev 0.5992184281349182
Epoch   1: Test 0.6080148220062256
patience: 0 / 5
Epoch   2  Train Loss 14.4836 91.60%
Epoch   2:  Dev 0.6040770411491394
Epoch   2: Test 0.608985960483551
patience: 0 / 5
Epoch   3  Train Loss 15.5268 91.94%
Epoch   3:  Dev 0.6000609397888184
patience: 1 / 5
Epoch   4  Train Loss 16.3321 92.20%
Epoch   4:  Dev 0.6036009192466736
patience: 2 / 5
Epoch   5  Train Loss 17.1497 92.37%
Epoch   5:  Dev 0.6056757569313049
Epoch   5: Test 0.6087661385536194
patience: 0 / 5
Epoch   6  Train Loss 17.8788 92.54%
Epoch   6:  Dev 0.5986352562904358
patience: 1 / 5
Epoch   7  Train Loss 18.4634 92.73%
Epoch   7:  Dev 0.6026057600975037
patience: 2 / 5
Epoch   8  Train Loss 19.1818 92.79%
Epoch   8:  Dev 0.59876948595047
patience: 3 / 5
Epoch   9  Train Loss 19.8724 92.90%
Epoch   9:  Dev 0.6011708378791809
patience: 4 / 5
Epoch  10  Train Loss 20.4113 93.05%
Epoch  10:  Dev 0.5969246625900269
patience: 5 / 5
setting train exemplar for learned classes
torch.Size([3360, 2048])
BEST DEV 4: 0.6056757569313049
BEST TEST 4: 0.6087661385536194
